<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>When Does Tabular RL Actually Break? | Zachary Burton</title>
  <meta name="author" content="Zachary Burton">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">

  <style>
    body {
      background-color: #ffffff;
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      margin-left: auto;
      margin-right: auto;
      width: 100%;
      max-width: 800px;
      padding: 20px;
      line-height: 1.6;
      color: #333;
    }
    h1, h2, h3, h4 { font-weight: 400; color: #333; }
    h1 { font-size: 28px; font-family: "Courier New", Courier, monospace; font-weight: bold; margin-bottom: 5px; }
    h2 { font-size: 20px; margin-bottom: 10px; margin-top: 35px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
    h3 { font-size: 16px; margin-bottom: 10px; margin-top: 25px; }
    p { line-height: 1.6; margin-bottom: 15px; }
    a { color: #1772d0; text-decoration: none; }
    a:hover { text-decoration: underline; }
    strong { font-weight: bold; }
    em { font-style: italic; }
    code {
      background: #f5f5f5;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: "Courier New", Courier, monospace;
      font-size: 13px;
    }
    pre {
      background: #f5f5f5;
      padding: 15px;
      border-radius: 4px;
      overflow-x: auto;
      font-family: "Courier New", Courier, monospace;
      font-size: 13px;
      line-height: 1.4;
      margin: 20px 0;
    }
    pre code { background: none; padding: 0; }
    .meta { color: #666; font-size: 13px; margin-bottom: 25px; }
    .tldr {
      background: #f8f8f8;
      border-left: 3px solid #1772d0;
      padding: 15px 20px;
      margin: 25px 0;
    }
    .tldr strong { color: #1772d0; }
    .key-finding {
      background: #fffbf0;
      border-left: 3px solid #e6a000;
      padding: 15px 20px;
      margin: 25px 0;
    }
    .key-finding strong { color: #b37800; }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 14px;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 10px 12px;
      text-align: left;
    }
    th { background: #f5f5f5; font-weight: bold; }
    tr:hover { background: #fafafa; }
    ul, ol { margin: 15px 0; padding-left: 25px; }
    li { margin-bottom: 8px; }
    .figure {
      text-align: center;
      margin: 30px 0;
    }
    .figure img {
      max-width: 100%;
      border: 1px solid #eee;
      border-radius: 4px;
    }
    .caption {
      font-size: 13px;
      color: #666;
      font-style: italic;
      margin-top: 10px;
    }
    .back-link { font-size: 13px; color: #666; }
    .back-link:hover { color: #1772d0; }
    hr { border: none; border-top: 1px solid #eee; margin: 40px 0; }
    .footer { text-align: center; color: #666; font-size: 13px; margin-top: 40px; }
  </style>
</head>

<body>
  <p><a href="/" class="back-link">&larr; Back to Home</a></p>

  <h1>When Does Tabular RL Actually Break?</h1>
  <p style="color: #666; font-style: italic; margin-bottom: 5px;">Testing representation bottlenecks on high school algebra</p>
  <p class="meta">Zac Burton &middot; December 31, 2025</p>

  <div class="tldr">
    <strong>TL;DR:</strong> AlphaProof solves IMO problems, but can vanilla RL solve <code>ax + b = c</code>? We systematically test tabular Q-learning, SARSA, PPO, and neural baselines to find the exact complexity threshold where RL breaks. Spoiler: it's embarrassingly low&mdash;depth-2 equations (~8th grade algebra). Even with 50,000 training episodes and neural encoders, solve rates stay below 5%. The problem isn't sample complexity&mdash;it's that flat representations can't capture compositional structure.
  </div>

  <h2>The Question</h2>

  <p>Modern AI systems like AlphaProof, Harmonic's Aristotle, and AxiomMath's AxiomProver can tackle most International Math Olympiad or Putnam problems alike, with ease. Meanwhile, I wanted to know: <strong>what's the simplest math problem where reinforcement learning completely falls apart?</strong></p>

  <p>Not because scaling up is impossible (obviously throw enough compute and you can brute-force anything), but because I wanted to understand the <em>fundamental</em> failure modes. Where does the representation break? Is it a data problem or an architecture problem?</p>

  <p>So I picked the most trivial symbolic task I could think of: <strong>solving single-variable linear equations</strong>.</p>

  <pre>2x = 4        # depth-1: one operation
3x + 5 = 11   # depth-2: two operations
2x - 3 = x + 7  # depth-3: three operations</pre>

  <p>The task: given an equation, apply algebraic rewrite rules (like "move constant across equality" or "divide both sides by coefficient") until you isolate <code>x</code>.</p>

  <p>High schoolers solve depth-3 equations in their sleep. Can RL?</p>

  <h2>The Setup</h2>

  <h3>Environment</h3>

  <p>Each equation is represented as an abstract syntax tree (AST). The agent sees the equation as a string (e.g., <code>"Equals(Mul(Const(2), Var(x)), Const(4))"</code>) and picks from a set of rewrite rules:</p>

  <ul>
    <li><code>divide_linear</code>: <code>kx = c</code> &rarr; <code>x = c/k</code></li>
    <li><code>move_const_l_to_r</code>: <code>x + b = c</code> &rarr; <code>x = c - b</code></li>
    <li><code>combine_like_terms</code>: <code>ax + bx</code> &rarr; <code>(a+b)x</code></li>
    <li>...and a few more</li>
  </ul>

  <p>Episodes end when the agent reaches a solved form (<code>x = k</code>) or hits 100 steps. Reward is +1 for solving, 0 otherwise.</p>

  <h3>Agents</h3>

  <p>I tested four approaches:</p>

  <ol>
    <li><strong>Tabular Q-learning</strong>: Standard &epsilon;-greedy with string-based state representation</li>
    <li><strong>SARSA</strong>: On-policy variant, same representation</li>
    <li><strong>PPO</strong>: Policy gradient with GRU encoder over the stringified AST</li>
    <li><strong>Random baseline</strong>: Uniform sampling over valid actions</li>
  </ol>

  <p>All agents use <strong>action masking</strong> to only select valid rewrites at each step.</p>

  <h2>Initial Results: Complete Failure at Depth-2</h2>

  <p>I trained each agent for 20,000 episodes on equations of varying depths:</p>

  <table>
    <thead>
      <tr>
        <th>Agent</th>
        <th>Depth-1</th>
        <th>Depth-2</th>
        <th>Depth-3</th>
        <th>Depth-4</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>PPO</strong></td>
        <td>38%</td>
        <td>19%</td>
        <td>4%</td>
        <td>1%</td>
      </tr>
      <tr>
        <td><strong>Q-Learning</strong></td>
        <td>4%</td>
        <td>5%</td>
        <td>4%</td>
        <td>1%</td>
      </tr>
      <tr>
        <td><strong>SARSA</strong></td>
        <td>4%</td>
        <td>5%</td>
        <td>4%</td>
        <td>1%</td>
      </tr>
      <tr>
        <td><strong>Random</strong></td>
        <td>4%</td>
        <td>5%</td>
        <td>1%</td>
        <td>1%</td>
      </tr>
    </tbody>
  </table>

  <div class="key-finding">
    <strong>Key Finding:</strong> Tabular Q-learning and SARSA are <em>statistically indistinguishable from random</em> across all depths. They never learn anything.
  </div>

  <p>PPO does better on depth-1 (38%), but by depth-3 it's back to random performance. And depth-2? A measly 19%.</p>

  <p>Why? The obvious hypothesis: <strong>state sparsity</strong>. Because states are represented as raw strings, <code>"Equals(Mul(Const(2), Var(x)), Const(4))"</code> and <code>"Equals(Mul(Const(3), Var(x)), Const(6))"</code> are treated as completely unrelated&mdash;even though they have identical solution structure.</p>

  <h2>The Investigation: Data vs. Representation</h2>

  <p>But wait&mdash;is this a <em>data problem</em> or a <em>representation problem</em>?</p>

  <p>To test this, I ran two follow-up experiments:</p>

  <h3>Experiment 1: Coefficient Normalization</h3>

  <p>What if we explicitly remove the coefficient information? Instead of treating <code>2x = 4</code> and <code>3x = 6</code> as different states, normalize them both to <code>C&middot;x = C</code>.</p>

  <p>This should massively reduce state space. For depth-1 equations, the state space collapses from ~341 unique states down to just 2.</p>

  <table>
    <thead>
      <tr>
        <th>Method</th>
        <th>Depth</th>
        <th>Episodes</th>
        <th>Solve Rate</th>
        <th>Q-Table Size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Regular Q-Learning</strong></td>
        <td>1</td>
        <td>20K</td>
        <td>5.4% &plusmn; 1.4%</td>
        <td>341 &plusmn; 7</td>
      </tr>
      <tr>
        <td></td>
        <td>2</td>
        <td>50K</td>
        <td>1.3% &plusmn; 0.8%</td>
        <td>673 &plusmn; 13</td>
      </tr>
      <tr>
        <td><strong>Normalized Q-Learning</strong></td>
        <td>1</td>
        <td>20K</td>
        <td>5.4% &plusmn; 1.4%</td>
        <td><strong>2 &plusmn; 0</strong></td>
      </tr>
      <tr>
        <td></td>
        <td>2</td>
        <td>50K</td>
        <td>0.5% &plusmn; 1.0%</td>
        <td><strong>8 &plusmn; 0</strong></td>
      </tr>
    </tbody>
  </table>

  <div class="key-finding">
    <strong>Result:</strong> Normalization achieves <strong>99% state space compression</strong> (341 &rarr; 2 states for depth-1). But performance is <em>identical</em>. Both methods completely fail on depth-2 (&lt;2% solve rate) even with 50K episodes.
  </div>

  <div class="figure">
    <img src="images/extended_learning_curves.png" alt="Learning curves showing plateau">
    <p class="caption">Learning curves over 50K episodes. Both regular and normalized Q-learning plateau at &lt;2% on depth-2.</p>
  </div>

  <h3>Experiment 2: Neural Baseline (GRU Encoder)</h3>

  <p>Maybe the problem is that <em>any</em> flat string representation is doomed. What if we let the agent learn its own representation?</p>

  <ul>
    <li><strong>Character-level GRU encoder</strong>: Maps equation strings to 64-dim embeddings</li>
    <li><strong>MLP Q-network</strong>: Predicts Q-values from (state embedding, action encoding)</li>
    <li><strong>Experience replay</strong>: Batch size 32, buffer size 10K</li>
  </ul>

  <table>
    <thead>
      <tr>
        <th>Method</th>
        <th>Depth-1</th>
        <th>Depth-2</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Tabular Q-Learning</td>
        <td>5%</td>
        <td>1%</td>
      </tr>
      <tr>
        <td><strong>Neural Q-Learning (GRU)</strong></td>
        <td><strong>10%</strong></td>
        <td><strong>4%</strong></td>
      </tr>
    </tbody>
  </table>

  <div class="key-finding">
    <strong>Result:</strong> Neural encoding provides 2&ndash;4&times; improvement, but 4% on depth-2 is still a <em>complete failure</em>.
  </div>

  <h2>Why Does This Happen?</h2>

  <p>The extended experiments rule out sample complexity. Even with 50,000 training episodes, 99% state space compression, and learned neural representations, agents still can't solve depth-2 equations.</p>

  <p>The problem is <strong>representations that don't capture compositional structure</strong>.</p>

  <p>Because <code>ax + b = c</code> requires a <em>sequence of dependent operations</em>:</p>

  <ol>
    <li>Move <code>b</code> to right side &rarr; <code>ax = c - b</code></li>
    <li>Divide by <code>a</code> &rarr; <code>x = (c - b)/a</code></li>
  </ol>

  <p>A flat string representation can't expose this dependency. The agent has no way to know that "the thing I do now affects what rewrites are possible later."</p>

  <h2>What Would Actually Work?</h2>

  <p>Based on these results, here's what you'd need to solve even depth-2 equations with RL:</p>

  <ul>
    <li><strong>Tree-structured encoder</strong>: Tree-LSTM or GNN over the AST</li>
    <li><strong>Symbolic priors</strong>: Hard-coded knowledge of algebraic equivalences</li>
    <li><strong>Hierarchical policies</strong>: "First isolate the variable, then simplify"</li>
    <li><strong>Search guidance</strong>: MCTS or beam search instead of pure RL</li>
  </ul>

  <p>This is exactly what AlphaProof and similar systems do. They don't use vanilla RL&mdash;they use heavily structured architectures with symbolic reasoning baked in.</p>

  <h2>Key Takeaways</h2>

  <ol>
    <li><strong>Tabular RL breaks at embarrassingly low complexity</strong>: Depth-2 equations (8th grade algebra) are essentially unsolvable.</li>
    <li><strong>It's not a data problem</strong>: 50K episodes, 99% state compression, and neural encoders all fail. The issue is representational.</li>
    <li><strong>Flat representations can't capture composition</strong>: String-based or sequence-based encodings miss the hierarchical structure needed for multi-step reasoning.</li>
    <li><strong>This motivates structured architectures</strong>: Modern neuro-symbolic systems succeed because they incorporate tree encoders, symbolic priors, and search.</li>
  </ol>

  <h2>Code & Reproducibility</h2>

  <p>All code is available on <a href="https://github.com/zacn04">GitHub</a>. The experiments ran on Azure ML with H100 GPUs (though most runs were CPU-only). Total compute cost: ~$170 for the extended experiments.</p>

  <hr>

  <p class="footer">
    <em>This work originated as a final project for 6.7920 (Reinforcement Learning, Theory and Foundations).</em><br>
    <a href="/">&larr; Back to Home</a>
  </p>
  <script data-goatcounter="https://zacn04.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</body>

</html>
